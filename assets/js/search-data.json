{
  
    
        "post0": {
            "title": "COUGH ONSET AND COUGH COUNTING.",
            "content": "As cough is a &#39;burst&#39; signal, with a sudden rise in the intensity level. Hence, a sudden temporal change can be witnessed both in the time as well as time-frequency domain. . In the Coswara data, the human annotations are peformed more or less denoting cough bouts. Each bout of cough can have 1 to &#39;n&#39; number of continuous streams of coughs.On detecting the onsets of coughs within these bouts, indirectly,we are also detecting the number of coughs. . This has task has been demonstrated on 3 sample files . import os import pandas as pd file_list = glob.glob(&#39;both/*&#39;) id_list = [] for file in file_list: id_list = id_list + [os.path.basename(file)] #coughwavefiles wavefile_list=[] for ids in id_list: wavefile_list = wavefile_list + [&#39;bothaudio/&#39;+ids+&#39;/cough-heavy.wav&#39;] json_list = [] for ji in id_list: json_list=json_list + [&#39;both/&#39;+ji+&#39;/cough-heavy_v2.json&#39;] with open(&#39;ids.txt&#39;, &#39;wb&#39;) as i: pickle.dump(id_list, i) . Example file 1 . x, sr=librosa.load(wavefile_list[6],sr=16000) librosa.display.waveplot(x) . &lt;matplotlib.collections.PolyCollection at 0x7f92146eb3c8&gt; . I have taken the second annotated region for demonstration. . import librosa.display import matplotlib.pyplot as plt sr=16000 with open(json_list[6]) as jsonfile: data = js.load(jsonfile) del data[&#39;vol&#39;] del data[&#39;stage&#39;] del data[&#39;cont&#39;] del data[&#39;annotator_name&#39;] del data[&#39;quality&#39;] librosa.display.waveplot(x[int(data[&#39;start_3&#39;]*sr):int(data[&#39;end_3&#39;]*sr)],sr) coughbout=x[int(data[&#39;start_3&#39;]*sr):int(data[&#39;end_3&#39;]*sr)] . The onset is calculated based on spectral flux of the power spectrogram in a given window. For a large/sudden variation, there will be a huge peak for those few time-frames in which the sudden changes are occuring. Here, the window is taken to be 512 sample window, with hop_length as 32 samples . onset_env = librosa.onset.onset_strength(y=coughbout, sr=sr, lag=1, power=2.0, n_fft=512, hop_length=32) plt.plot(onset_env) #print(len(onset_env)) #print((onset_env)) . /home/ananya/anaconda3/lib/python3.7/site-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels. &#34;Empty filters detected in mel frequency basis. &#34; . [&lt;matplotlib.lines.Line2D at 0x7f9214640cc0&gt;] . As can be seen in the above plot, 3 prominent peak can be visualised. There are numerous other peaks too. Hence, to pick the right peaks, peak picking based on onset detection is used, which gives a coarse estimation of the time frames. . times = librosa.times_like(onset_env, sr=sr) onset_f = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr) print(onset_f) plt.plot(onset_env) plt.vlines(onset_f,0, onset_env.max(),color=&#39;black&#39;, alpha=0.8, linestyle=&#39;dotted&#39;) print(times[onset_f]) . [ 28 29 30 336 337 338] [ 0.896 0.928 0.96 10.752 10.784 10.816] . To refine this estimation of peaks(true onsets), peak picking is performed based on &#39;prominence&#39; criteria of the onset-strength envelope, wherein,the top 5% quantile peak values for a given audio segment, only are taken into consideration. As cough values can be considered as bursts, this statistic gives decent results. . from scipy.signal import find_peaks peaks,_ = find_peaks(onset_env, prominence=np.quantile(onset_env,0.95)) plt.plot(onset_env) plt.plot(peaks, onset_env[peaks], &#39;ob&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f92145d5860&gt;] . The &#39;peaks&#39; parameter consequently gives the frame at which the peak occurs.The timing instants obtained from onset envelopes is divided by a factor of 16 due to the values of the parameters: window_size=512 and hop_length=32. (512/32=16). This gives us the true instants(in seconds) of the onset of a single cough. . print(peaks) print(times[peaks]/16) . [ 29 337] [0.058 0.674] . I have performed similar operations on two more audio segments for better clarity. . Example file 2 . x, sr=librosa.load(wavefile_list[0],sr=16000) librosa.display.waveplot(x) . &lt;matplotlib.collections.PolyCollection at 0x7f9214cf1fd0&gt; . import librosa.display import matplotlib.pyplot as plt sr=16000 with open(json_list[0]) as jsonfile: data = js.load(jsonfile) del data[&#39;vol&#39;] del data[&#39;stage&#39;] del data[&#39;cont&#39;] del data[&#39;annotator_name&#39;] del data[&#39;quality&#39;] librosa.display.waveplot(x[int(data[&#39;start_1&#39;]*sr):int(data[&#39;end_1&#39;]*sr)],sr) coughbout=x[int(data[&#39;start_1&#39;]*sr):int(data[&#39;end_1&#39;]*sr)] . onset_env = librosa.onset.onset_strength(y=coughbout, sr=sr, lag=1, power=2.0, n_fft=512, hop_length=32) plt.plot(onset_env) plt.title(&#39;Onsets of cough&#39;) #print(len(onset_env)) #print((onset_env)) . /home/ananya/anaconda3/lib/python3.7/site-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels. &#34;Empty filters detected in mel frequency basis. &#34; . Text(0.5, 1.0, &#39;Onsets of cough&#39;) . times = librosa.times_like(onset_env, sr=sr) onset_f = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr) print(onset_f) plt.plot(onset_env, color = &#39;green&#39;) plt.vlines(onset_f,0, onset_env.max(),color=&#39;black&#39;, alpha=0.8, linestyle=&#39;dotted&#39;) print(times[onset_f]) . [ 13 16 17 18 21 277 278 511 512] [ 0.416 0.512 0.544 0.576 0.672 8.864 8.896 16.352 16.384] . from scipy.signal import find_peaks peaks,_ = find_peaks(onset_env, prominence=np.quantile(onset_env,0.95)) plt.plot(onset_env, color = &#39;purple&#39;) plt.plot(peaks, onset_env[peaks], &#39;ob&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f921672b400&gt;] . Example file 3 . x, sr=librosa.load(wavefile_list[1],sr=16000) librosa.display.waveplot(x) . &lt;matplotlib.collections.PolyCollection at 0x7f9216b4eb00&gt; . I have chosen the first segment for demostration purpose again. . import librosa.display import matplotlib.pyplot as plt sr=16000 with open(json_list[1]) as jsonfile: data = js.load(jsonfile) del data[&#39;vol&#39;] del data[&#39;stage&#39;] del data[&#39;cont&#39;] del data[&#39;annotator_name&#39;] del data[&#39;quality&#39;] librosa.display.waveplot(x[int(data[&#39;start_1&#39;]*sr):int(data[&#39;end_1&#39;]*sr)],sr) coughbout=x[int(data[&#39;start_1&#39;]*sr):int(data[&#39;end_1&#39;]*sr)] . onset_env = librosa.onset.onset_strength(y=coughbout, sr=sr, lag=1, power=2.0, n_fft=512, hop_length=32) plt.plot(onset_env) plt.title(&#39;Onsets of cough&#39;) . /home/ananya/anaconda3/lib/python3.7/site-packages/librosa/filters.py:239: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels. &#34;Empty filters detected in mel frequency basis. &#34; . Text(0.5, 1.0, &#39;Onsets of cough&#39;) . times = librosa.times_like(onset_env, sr=sr) onset_f = librosa.onset.onset_detect(onset_envelope=onset_env, sr=sr) print(onset_f) plt.plot(onset_env, color = &#39;green&#39;) plt.vlines(onset_f,0, onset_env.max(),color=&#39;black&#39;, alpha=0.8, linestyle=&#39;dotted&#39;) print(times[onset_f]) . [19 20 21] [0.608 0.64 0.672] . from scipy.signal import find_peaks peaks,_ = find_peaks(onset_env, prominence=np.quantile(onset_env,0.95)) plt.plot(onset_env, color = &#39;purple&#39;) plt.plot(peaks, onset_env[peaks], &#39;ob&#39;) . [&lt;matplotlib.lines.Line2D at 0x7f92147ad5c0&gt;] .",
            "url": "https://ananyamuguli.github.io/Blogs/2020/10/16/Cough-onset-and-counting.html",
            "relUrl": "/2020/10/16/Cough-onset-and-counting.html",
            "date": " • Oct 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "HEAVY-COUGH 1D Feature analysis",
            "content": "import pandas as pd import numpy as np import glob import os file_list = glob.glob(&#39;both/*&#39;) id_list = [] for file in file_list: id_list = id_list + [os.path.basename(file)] audio_files=[] for ids in id_list: audio_files = audio_files + [&#39;bothaudio/&#39;+ids+&#39;/cough-heavy.wav&#39;] all_files_js=[] for ids in id_list: all_files_js = all_files_js + [&#39;both/&#39;+ids+&#39;/cough-heavy_v2.json&#39;] . import matplotlib.pyplot as plt import json as js import librosa import math from scipy.stats import skew from scipy.stats import kurtosis import seaborn as sns import matplotlib.pyplot as plt import pickle as pkl from sklearn.preprocessing import StandardScaler from sklearn.decomposition import PCA d = pd.read_csv(&#39;cough_params.csv&#39;) def spectralcentroid(y,l, da, sr): spr = librosa.feature.spectral_centroid(y=y[math.ceil(da[&#39;start_{}&#39;.format(l+1)]*sr):math.ceil(sr*da[&#39;end_{}&#39;.format(l+1)])],sr=sr,n_fft=256, hop_length=128) return spr def zcr(y,l,da,sr): spr =(librosa.feature.zero_crossing_rate(y=y[math.ceil(da[&#39;start_{}&#39;.format(l+1)]*sr):math.ceil(sr*da[&#39;end_{}&#39;.format(l+1)])],frame_length=256, hop_length=128)) return spr def rmse(y,l,da,sr): spr = (librosa.feature.rms(y=y[math.ceil(da[&#39;start_{}&#39;.format(l+1)]*sr):math.ceil(sr*da[&#39;end_{}&#39;.format(l+1)])],frame_length=256, hop_length=128)) return spr def spectralflatness(y,l,da,sr): spr = librosa.feature.spectral_flatness(y=y[math.ceil(da[&#39;start_{}&#39;.format(l+1)]*sr):math.ceil(sr*da[&#39;end_{}&#39;.format(l+1)])],n_fft=256, hop_length=128) return spr def spectralrolloff(y,l,da,sr): spr = librosa.feature.spectral_rolloff(y=y[math.ceil(da[&#39;start_{}&#39;.format(l+1)]*sr):math.ceil(sr*da[&#39;end_{}&#39;.format(l+1)])],sr=sr,n_fft=256, hop_length=128) return spr def feature_function(feature_name, tag): d = pd.read_csv(&#39;cough_params.csv&#39;) globals()[&#39;data_{}&#39;.format(tag)]=pd.DataFrame(columns=[&#39;ID&#39;,&#39;status&#39;,&#39;{}_mean&#39;.format(tag),&#39;{}_median&#39;.format(tag),&#39;{}_std&#39;.format(tag),&#39;{}_var&#39;.format(tag),&#39;{}_min&#39;.format(tag),&#39;{}_max&#39;.format(tag),&#39;{}_quartile25&#39;.format(tag),&#39;{}_quartile75&#39;.format(tag),&#39;{}_kurtosis&#39;.format(tag),&#39;{}_skewness&#39;.format(tag)]) globals()[&#39;data_{}&#39;.format(tag)][[&#39;ID&#39;,&#39;status&#39;]] = d[[&#39;ID&#39;,&#39;status&#39;]] globals()[&#39;{}_file&#39;.format(tag)] = open(&#39;{}_averages.txt&#39;.format(tag),&#39;a+&#39;) for idno in range(len(id_list)): with open(all_files_js[idno]) as jsonfile: da = js.load(jsonfile) del da[&#39;vol&#39;] del da[&#39;stage&#39;] del da[&#39;cont&#39;] del da[&#39;annotator_name&#39;] del da[&#39;quality&#39;] avg = np.zeros((20)) y, sr = librosa.load(audio_files[idno], sr = 16000) for l in range(int(len(da)/2)): if(feature_name == &#39;spectral_centroid&#39;): spr = spectralcentroid(y,l,da,sr) elif(feature_name == &#39;spectral_flatness&#39;): spr = spectralflatness(y,l,da,sr) elif(feature_name == &#39;rmse&#39;): spr = rmse(y,l,da,sr) elif(feature_name == &#39;zcr&#39;): spr = zcr(y,l,da,sr) elif(feature_name == &#39;spectral_rolloff&#39;): spr = spectralrolloff(y,l,da,sr) else: print(&#39;wrong feature input&#39;) break if(len(avg)&lt;len(spr[0])): d=len(spr[0])-len(avg) zc= np.pad(avg, pad_width =(0,d), mode =&#39;constant&#39;)[0] avg = (zc+spr[0])*0.5 elif (len(spr[0])&lt;len(avg)): d= len(avg)-len(spr[0]) zc =np.pad(spr[0], pad_width=(0,d), mode=&#39;constant&#39;) avg = (zc+avg)*0.5 else: avg =(spr[0]+avg)*0.5 globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_mean&#39;.format(tag)][idno]=np.average(avg) globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_median&#39;.format(tag)][idno]=np.median(avg) globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_std&#39;.format(tag)][idno]=np.std(avg) globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_var&#39;.format(tag)][idno]=np.var(avg) globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_min&#39;.format(tag)][idno]=np.amin(avg) globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_max&#39;.format(tag)][idno]=np.amax(avg) globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_quartile25&#39;.format(tag)][idno]=np.quantile(avg,0.25) globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_quartile75&#39;.format(tag)][idno]=np.quantile(avg,0.75) globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_kurtosis&#39;.format(tag)][idno]=kurtosis(avg) globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_skewness&#39;.format(tag)][idno]=skew(avg) globals()[&#39;{}_file&#39;.format(tag)].write(id_list[idno]) globals()[&#39;{}_file&#39;.format(tag)].write(&#39;: t&#39;) globals()[&#39;{}_file&#39;.format(tag)].write(str(list(avg))) globals()[&#39;{}_file&#39;.format(tag)].write(&#39; n&#39;) globals()[&#39;col_{}&#39;.format(tag)]=[&#39;{}_mean&#39;.format(tag),&#39;{}_median&#39;.format(tag),&#39;{}_std&#39;.format(tag),&#39;{}_var&#39;.format(tag),&#39;{}_min&#39;.format(tag),&#39;{}_max&#39;.format(tag),&#39;{}_quartile25&#39;.format(tag),&#39;{}_quartile75&#39;.format(tag),&#39;{}_kurtosis&#39;.format(tag),&#39;{}_skewness&#39;.format(tag)] globals()[&#39;data_{}&#39;.format(tag)].to_csv(&#39;{}features.csv&#39;.format(tag)) globals()[&#39;data_{}&#39;.format(tag)][globals()[&#39;col_{}&#39;.format(tag)]] =(globals()[&#39;data_{}&#39;.format(tag)][globals()[&#39;col_{}&#39;.format(tag)]]).astype(float) globals()[&#39;{}_matrix&#39;.format(tag)] = globals()[&#39;data_{}&#39;.format(tag)][globals()[&#39;col_{}&#39;.format(tag)]].to_numpy() np.save(&#39;{}_featurematrix.npy&#39;.format(tag),globals()[&#39;{}_matrix&#39;.format(tag)]) #pklfile = open(&#39;{}_picklefile.pkl&#39;.format(tag), &#39;wb&#39;) #pkl.dump(globals()[&#39;{}_file&#39;.format(tag)],pklfile) def plotavgfunction(tag, feature_name): plt.figure(figsize=(20,20)) plt.plot(globals()[&#39;data_{}&#39;.format(tag)][&#39;{}_mean&#39;.format(tag)]) plt.title(&#39;Plotting average distribution of {}&#39;.format(feature_name)) def corrfunction(feature_name,data,col): plt.figure(figsize=(20,20)) sns.heatmap(data[col].corr(), annot=True, annot_kws={&quot;fontsize&quot;:18}) sns.set(font_scale=2) plt.title(&#39;{} features correlation matrix&#39;.format(feature_name)) plt.savefig(&#39;{}_correlationmatrixfeatures.png&#39;.format(feature_name)) def pairplotfunction(feature_name,data): sns.pairplot(data, kind=&quot;scatter&quot;, hue=&quot;status&quot;) plt.savefig(&#39;SpectralFlatnessfeaturesrelation.png&#39;) def PCAfunction(tag,feature_name): globals()[&#39;{}_scaled&#39;.format(tag)] = StandardScaler().fit_transform(globals()[&#39;{}_matrix&#39;.format(tag)]) globals()[&#39;{}_scaled_df&#39;.format(tag)] = pd.DataFrame(sr_scaled, columns=globals()[&#39;col_{}&#39;.format(tag)]) pca = PCA(n_components=10) principal_comps=pca.fit_transform(globals()[&#39;{}_scaled&#39;.format(tag)]) globals()[&#39;{}_pca_df&#39;.format(tag)] = pd.DataFrame(data=principal_comps, columns=[&#39;PCA1&#39;,&#39;PCA2&#39;,&#39;PCA3&#39;,&#39;PCA4&#39;,&#39;PCA5&#39;,&#39;PCA6&#39;,&#39;PCA7&#39;,&#39;PCA8&#39;,&#39;PCA9&#39;,&#39;PCA10&#39;]) globals()[&#39;pcavar_{}&#39;.format(tag)]=pca.explained_variance_ratio_ globals()[&#39;ev_{}&#39;.format(tag)] = pca.singular_values_ plt.figure(figsize=(20,10)) plt.grid(True) plt.plot(globals()[&#39;ev_{}&#39;.format(tag)] ,color=&#39;k&#39;, marker=&#39;o&#39;) for num in range(len(globals()[&#39;ev_{}&#39;.format(tag)] )): plt.annotate(round(globals()[&#39;ev_{}&#39;.format(tag)][num],2),(num, globals()[&#39;ev_{}&#39;.format(tag)][num])) plt.title(&#39;Eigen values for {} features&#39;.format(feature_name)) plt.savefig(&#39;PCA{}EigVal.png&#39;.format(tag)) plt.show() def PCAvar(tag, feature_name): plt.figure(figsize=(20,10)) plt.grid(True) plt.plot(globals()[&#39;pcavar_{}&#39;.format(tag)],color=&#39;k&#39;, marker=&#39;o&#39;) for num in range(len(globals()[&#39;pcavar_{}&#39;.format(tag)])): plt.annotate(round(globals()[&#39;pcavar_{}&#39;.format(tag)][num],3),(num,globals()[&#39;pcavar_{}&#39;.format(tag)][num])) plt.title(&#39;variance explained by each PCA component for {}.&#39;.format(tag)) plt.savefig(&#39;PCA{}var.png&#39;.format(tag)) plt.show() . 1 features calculated are:- ZCR, spectral Centroid, RMSE, Spectral Flatness, Spectral RollOff, MFCCs and delta-MFCCs . The 1D features (time series) (RMSE, ZCR, SR, SF, SC) extracted from the annotations boundaries for each ID is averaged, and the statistical parameters of these averaged features are extracted, namely:- 1)Mean 2)Median 3)Standard Deviation 4)Variance 5)min value 6)Max value 7)25% quantile 8)75% quantile 9)kurtosis 10)Skewness We get a 100* 10feature matrix for each 1D feature. . For MFCC and delta-MFCC, first 13 coefficeints are averaged for each ID. Each of these feature matrices form 100* 13 feature matrix . Combing all these features, we have a feature meatrix of 100* 76. PCA was performed on it, and the variance was plotted. . The correlation plot shows the correaltion values between different statistical features. . The pairplot functions show the relationship between the data points of two variables for a given ID. . Spectral Centroid . feature_function(&#39;spectral_centroid&#39;,&#39;sc&#39;) . corrfunction(&#39;spectral_centroid&#39;,data_sc,col_sc) . pairplotfunction(&#39;spectral_centroid&#39;,data_sc) . PCAfunction(&#39;sc&#39;,&#39;spectral_centroid&#39;) . PCAvar(&#39;sc&#39;, &#39;spectral_centroid&#39;) . For the Spectral Centroid features nearly 75% of the variance is explained by the first two PCA components. . Spectral Flatness . feature_function(&#39;spectral_flatness&#39;,&#39;sf&#39;) . corrfunction(&#39;spectral_flatness&#39;,data_sf,col_sf) . pairplotfunction(&#39;spectral_flatness&#39;,data_sf) . PCAfunction(&#39;sf&#39;,&#39;spectral_flatness&#39;) . PCAvar(&#39;sf&#39;, &#39;spectral_flatness&#39;) . More than 80% of the variations is explained by the first two factors. . Spectral RollOff . feature_function(&#39;spectral_rolloff&#39;,&#39;sr&#39;) . corrfunction(&#39;spectral_rolloff&#39;,data_sr,col_sr) . pairplotfunction(&#39;spectral_rolloff&#39;,data_sr) . PCAfunction(&#39;sr&#39;,&#39;spectral_rolloff&#39;) . PCAvar(&#39;sr&#39;, &#39;spectral_rolloff&#39;) . Nearly 82% of the variance is explained by the first two components. . ZCR . feature_function(&#39;zcr&#39;,&#39;zcr&#39;) . corrfunction(&#39;zcr&#39;,data_zcr,col_zcr) . pairplotfunction(&#39;zcr&#39;,data_zcr) . PCAfunction(&#39;zcr&#39;,&#39;zcr&#39;) . PCAvar(&#39;zcr&#39;, &#39;zcr&#39;) . Nearly 77% of the variance is explained by the first two components . RMSE . feature_function(&#39;rmse&#39;,&#39;rmse&#39;) . corrfunction(&#39;rmse&#39;,data_rmse,col_rmse) . pairplotfunction(&#39;rmse&#39;,data_rmse) . PCAfunction(&#39;rmse&#39;,&#39;rmse&#39;) . PCAvar(&#39;rmse&#39;, &#39;rmse&#39;) . Nearly 76% of the variance is explained by the first two components. . sf_pca_df.to_csv(&#39;pcasf.csv&#39;) sc_pca_df.to_csv(&#39;pcasc.csv&#39;) sr_pca_df.to_csv(&#39;pcasr.csv&#39;) rmse_pca_df.to_csv(&#39;pcarmse.csv&#39;) zcr_pca_df.to_csv(&#39;pcazcr.csv&#39;) . MFCC&#39;s and d-MFCC&#39;s . The first 13 MFCC coeficcient for each frame is calculated. Delta-MFCC features primarily gives a description of the temporal information in the signal. . cols = [&#39;ID&#39;,&#39;status&#39;,&#39;MFCC0&#39;,&#39;MFCC1&#39;,&#39;MFCC2&#39;,&#39;MFCC3&#39;,&#39;MFCC4&#39;,&#39;MFCC5&#39;,&#39;MFCC6&#39;,&#39;MFCC7&#39;,&#39;MFCC8&#39;, &#39;MFCC9&#39;,&#39;MFCC10&#39;,&#39;MFCC11&#39;,&#39;MFCC12&#39;] dcols = [&#39;ID&#39;,&#39;status&#39;,&#39;dMFCC0&#39;,&#39;dMFCC1&#39;,&#39;dMFCC2&#39;,&#39;dMFCC3&#39;,&#39;dMFCC4&#39;,&#39;dMFCC5&#39;,&#39;dMFCC6&#39;,&#39;dMFCC7&#39;,&#39;dMFCC8&#39;, &#39;dMFCC9&#39;,&#39;dMFCC10&#39;,&#39;dMFCC11&#39;,&#39;dMFCC12&#39;] mfcc_df = pd.DataFrame(columns=cols) dmfcc_df = pd.DataFrame(columns=dcols) mfcc_df[[&#39;ID&#39;,&#39;status&#39;]] = d[[&#39;ID&#39;,&#39;status&#39;]] dmfcc_df[[&#39;ID&#39;,&#39;status&#39;]] = d[[&#39;ID&#39;,&#39;status&#39;]] . for idno in range(len(id_list)): with open(all_files_js[idno]) as jsonfile: da = js.load(jsonfile) del da[&#39;vol&#39;] del da[&#39;stage&#39;] del da[&#39;cont&#39;] del da[&#39;annotator_name&#39;] del da[&#39;quality&#39;] mfccavg =np.zeros(13) dmfccavg =np.zeros(13) l=0 y, sr = librosa.load(audio_files[idno], sr = 16000) for l in range(int(len(da)/2)): mfcc = [] dmfcc = [] MFCC = (librosa.feature.mfcc(y=y[math.ceil(da[&#39;start_{}&#39;.format(l+1)]*sr):math.ceil(sr*da[&#39;end_{}&#39;.format(l+1)])],sr=sr,n_fft=256, hop_length=128, n_mfcc = 13, n_mels=64)) dMFCC = librosa.feature.delta(MFCC) for i in range(13): mfcc = mfcc + [sum(list(MFCC[i]))/len(list(MFCC[i]))] dmfcc = dmfcc + [sum(list(dMFCC[i]))/len(list(dMFCC[i]))] mfcc=np.asarray(mfcc) dmfcc=np.asarray(dmfcc) mfccavg = (mfccavg+mfcc)*0.5 dmfccavg = (dmfccavg+dmfcc)*0.5 for i in range(13): mfcc_df[&#39;MFCC{}&#39;.format(i)][idno] = mfccavg[i] dmfcc_df[&#39;dMFCC{}&#39;.format(i)][idno] = dmfccavg[i] . mfcc_df.to_csv(&#39;MFCCfeatures.csv&#39;) dmfcc_df.to_csv(&#39;dMFCCfeatures.csv&#39;) . plt.figure(figsize=(10,10)) plt.plot(mfcc_df[&#39;MFCC0&#39;], label = &#39;mfcc0&#39;) plt.plot(mfcc_df[&#39;MFCC1&#39;], label = &#39;mfcc1&#39;) plt.plot(mfcc_df[&#39;MFCC2&#39;], label = &#39;mfcc2&#39;) plt.legend(loc = &quot;upper right&quot;)b . &lt;matplotlib.legend.Legend at 0x7fdde48c6e48&gt; . The MFCC[0] that represents the average energy of the audio signal has negative values for each user ID. This represents that most of the spectral energy for cough is concentrated in higher frequency regions. . 1D feature analysis . feature_df_1d=(((((mfcc_df.merge(dmfcc_df, how=&#39;outer&#39;)).merge(data_sc, how=&#39;outer&#39;)).merge(data_sr, how=&#39;outer&#39;) ).merge(data_zcr, how=&#39;outer&#39;)).merge(data_rmse, how=&#39;outer&#39;)).merge(data_sf, how=&#39;outer&#39;) feature_names=(feature_df_1d.columns[2:]) feature_df_1D = feature_df_1d[feature_names] feature_df_1D = feature_df_1D.astype(float) feature_matrix_1D=feature_df_1D.to_numpy() . features_scaled = StandardScaler().fit_transform(feature_matrix_1D) features_scaled_df = pd.DataFrame(features_scaled, columns=feature_names) pca = PCA(n_components=15) principal_comps=pca.fit_transform(features_scaled) #features1D_pca_df = pd.DataFrame(data=principal_comps, columns=[&#39;PCA1&#39;,&#39;PCA2&#39;,&#39;PCA3&#39;,&#39;PCA4&#39;,&#39;PCA5&#39;,&#39;PCA6&#39;,&#39;PCA7&#39;,&#39;PCA8&#39;,&#39;PCA9&#39;,&#39;PCA10&#39;]) pcavar_features=pca.explained_variance_ratio_ ev_features= pca.singular_values_ plt.figure(figsize=(20,10)) plt.grid(True) plt.plot(ev_features ,color=&#39;k&#39;, marker=&#39;o&#39;) for num in range(len(ev_features)): plt.annotate(round(ev_features[num],2),(num, ev_features[num])) plt.title(&#39;Eigen values for 1D cough features&#39;) plt.savefig(&#39;PCA1D_featuresEigVal.png&#39;) plt.show() . plt.figure(figsize=(20,10)) plt.grid(True) plt.plot(pcavar_features,color=&#39;k&#39;, marker=&#39;o&#39;) for num in range(len(pcavar_features)): plt.annotate(round(pcavar_features[num],3),(num,pcavar_features[num])) plt.title(&#39;variance explained by each PCA component&#39;) plt.savefig(&#39;PCAvar1Dfeatures.png&#39;) plt.show() . The PCA reduction of the entire feature matrix 100 * 76 gave us components that are not entirely dominant. The first two component hold only 34% of the variation information. .",
            "url": "https://ananyamuguli.github.io/Blogs/2020/10/14/cough1Dfeatureanalysis.html",
            "relUrl": "/2020/10/14/cough1Dfeatureanalysis.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "COUGH FEATURE ANALYSIS",
            "content": "import pandas as pd import numpy as np import glob import pickle #the ids of the coughs-deep.wave pid_list = glob.glob(&#39;positiveaudio/*&#39;) #coughwavefiles pwavefile_list=[] for ids in pid_list: pwavefile_list = pwavefile_list + [ids+&#39;/cough-heavy.wav&#39;] pjson_id = glob.glob(&#39;positive/*&#39;) pjson_list = [] for ji in pjson_id: pjson_list=pjson_list + [ji+&#39;/cough-heavy_v2.json&#39;] nid_list = glob.glob(&#39;negativeaudio/*&#39;) #coughwavefiles nwavefile_list=[] for ids in nid_list: nwavefile_list = nwavefile_list + [ids+&#39;/cough-heavy.wav&#39;] njson_id = glob.glob(&#39;negative/*&#39;) njson_list = [] for ji in njson_id: njson_list=njson_list + [ji+&#39;/cough-heavy_v2.json&#39;] with open(&#39;positiveids.txt&#39;, &#39;wb&#39;) as pi: pickle.dump(pid_list,pi) with open(&#39;negativeids.txt&#39;, &#39;wb&#39;) as ri: pickle.dump(nid_list, ri) . import os import pandas as pd file_list = glob.glob(&#39;both/*&#39;) id_list = [] for file in file_list: id_list = id_list + [os.path.basename(file)] #coughwavefiles wavefile_list=[] for ids in id_list: wavefile_list = wavefile_list + [&#39;bothaudio/&#39;+ids+&#39;/cough-heavy.wav&#39;] json_list = [] for ji in id_list: json_list=json_list + [&#39;both/&#39;+ji+&#39;/cough-heavy_v2.json&#39;] with open(&#39;ids.txt&#39;, &#39;wb&#39;) as i: pickle.dump(id_list, i) . import json as js metadata = [] for ids in id_list: metadata = metadata + [&#39;bothaudio/&#39;+ids+&#39;/metadata.json&#39;] with open(&#39;meadatapaths.txt&#39;, &#39;wb&#39;) as md: pickle.dump(metadata, md) df = pd.DataFrame(columns=[&#39;ID&#39;,&#39;gender&#39;,&#39;age&#39;,&#39;status&#39;,&#39;number of cycles&#39;,&#39;mean duration&#39;,&#39;avg time between cycles&#39;]) df[&#39;ID&#39;] = id_list for jl in range(len(metadata)): with open(metadata[jl]) as jsonfile: data = js.load(jsonfile) df[&#39;gender&#39;][jl] = data[&#39;g&#39;] df[&#39;age&#39;][jl] = data[&#39;a&#39;] if(data[&#39;covid_status&#39;] == &#39;healthy&#39;): df[&#39;status&#39;][jl] = &#39;non_covid&#39; elif(data[&#39;covid_status&#39;] == &#39;resp_illness_not_identified&#39;): df[&#39;status&#39;][jl] = &#39;non_covid&#39; elif(data[&#39;covid_status&#39;] == &#39;no_resp_illness_exposed&#39;): df[&#39;status&#39;][jl] = &#39;non_covid&#39; else: df[&#39;status&#39;][jl] = &#39;covid&#39; df[&#39;status&#39;].value_counts() . non_covid 80 covid 20 Name: status, dtype: int64 . 100 cough files are chosen that are reasonably clean. 80 non-covid and 20 covid cough samples are chosen. Four physical aspects of this data is introspected.They are:- 1)age 2)number of cough cycles per file (3 was asked of the patients, but there has been variations). 3)Duration of each cough cycle in covid and non-covid 4)Average duration between cough cycles for each ID. (In rare cases wherein only 1 cough bout has been detected, (such files being smaller than the usual), it is presumed that the next cycle is just after the end of the audio, and hence the average duration is length_of_audio_file-end_boundary . import librosa for jl in range(len(json_list)): with open(json_list[jl]) as jsonfile: data = js.load(jsonfile) del data[&#39;vol&#39;] del data[&#39;stage&#39;] del data[&#39;cont&#39;] del data[&#39;annotator_name&#39;] del data[&#39;quality&#39;] avg = 0 df[&#39;number of cycles&#39;][jl] = int(len(data)/2) for l in range(int(len(data)/2)): avg = (avg + (data[&#39;end_{}&#39;.format(l+1)]-data[&#39;start_{}&#39;.format(l+1)]))/2 df[&#39;mean duration&#39;][jl] = avg y,sr = librosa.load(wavefile_list[jl], sr = 16000) dur = librosa.get_duration(y=y, sr = sr) mean_time_between_cycles = 0 if(df[&#39;number of cycles&#39;][jl] &gt; 1): for l in range(int(len(data)/2)-1): mean_time_between_cycles = (mean_time_between_cycles + (data[&#39;start_{}&#39;.format(l+2)]-data[&#39;end_{}&#39;.format(l+1)]))/2 else: mean_time_between_cycles = dur - data[&#39;end_{}&#39;.format(l+1)] df[&#39;avg time between cycles&#39;][jl] = mean_time_between_cycles . df.to_csv(&#39;cough_params.csv&#39;) df.head() . ID gender age status number of cycles mean duration avg time between cycles . 0 IkdWn0v3dAU9eWzsf3rY8Dwgf0V2 | female | 20 | covid | 1 | 0.6605 | 0.208375 | . 1 iu13CF47sRTOnPlhkzTszNkqAZ83 | male | 29 | non_covid | 3 | 0.42 | 0.18775 | . 2 VB3X18XtrRdSVJ8zDzdyKohlX2F2 | female | 29 | non_covid | 8 | 0.432094 | 0.425813 | . 3 lzB1CcCk33VjfRiW9vbXkplPO0r1 | female | 23 | non_covid | 6 | 0.463938 | 0.315594 | . 4 uHPhyLsj5eYghJQOYj7iXa0fyOE3 | male | 30 | non_covid | 5 | 0.409156 | 0.771062 | . params = df.columns params . Index([&#39;ID&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;status&#39;, &#39;number of cycles&#39;, &#39;mean duration&#39;, &#39;avg time between cycles&#39;], dtype=&#39;object&#39;) . import seaborn as sns import matplotlib.pyplot as plt sns.displot(data = df, x = &#39;number of cycles&#39;, kind =&quot;hist&quot;, hue = &#39;status&#39;) plt.title(&#39;distribution of number of cycles&#39;) plt.savefig(&#39;numcycles.png&#39;) . As we can see, most of the recorded data(61) have 3 cycles per files. But there are outliers towards 1 cycle and 12 cycle. The covid positive data in these 100 samples closely follows the non-covid distribution, having considerable amount of data in 1/2 samples. . import seaborn as sns import matplotlib.pyplot as plt sns.stripplot(data = df, x = &#39;gender&#39;,y = &#39;mean duration&#39;, hue = &#39;status&#39;) plt.title(&#39;mean duration of each cough cycle&#39;) plt.savefig(&#39;meandurationofeachcycle.png&#39;) . The plot of mean duration of each cough cycle between male and female. . import seaborn as sns import matplotlib.pyplot as plt sns.jointplot(data = df, x = &#39;avg time between cycles&#39;,y = &#39;mean duration&#39;) #plt.title(&#39;relation between mean duration of each cough cycle to avg time between them.&#39;) plt.savefig(&#39;meandurationofeachcycle.png&#39;) . df[&#39;age&#39;] = df.age.astype(int) df[&#39;number of cycles&#39;] = df[&quot;number of cycles&quot;].astype(float) df[&#39;mean duration&#39;] = df[&quot;mean duration&quot;].astype(float) df[&#39;avg time between cycles&#39;] = df[&quot;avg time between cycles&quot;].astype(float) df[&#39;status&#39;] = df[&quot;status&quot;].astype(&#39;str&#39;) df[&#39;gender&#39;] = df[&quot;gender&quot;].astype(&#39;str&#39;) df.dtypes . ID object gender object age int64 status object number of cycles float64 mean duration float64 avg time between cycles float64 dtype: object . import seaborn as sns import matplotlib.pyplot as plt sns.jointplot(data = df, x = &#39;avg time between cycles&#39;,y = &#39;mean duration&#39;, hue = &#39;status&#39;) plt.savefig(&#39;huestatus.png&#39;) . import seaborn as sns import matplotlib.pyplot as plt sns.jointplot(data = df, x = &#39;avg time between cycles&#39;,y = &#39;mean duration&#39;, hue = &#39;gender&#39;) plt.savefig(&#39;huegender.png&#39;) . The above two plots show that there is some marginal correlation between the two variables that describe average time bewteen cough cycles and mean duration between cough cycles . df.describe() . age number of cycles mean duration avg time between cycles . count 100.000000 | 100.000000 | 100.000000 | 100.000000 | . mean 32.520000 | 3.410000 | 0.593892 | 0.545423 | . std 11.308779 | 1.621167 | 0.261956 | 0.310578 | . min 14.000000 | 1.000000 | 0.186625 | 0.043500 | . 25% 24.750000 | 3.000000 | 0.406688 | 0.313695 | . 50% 30.000000 | 3.000000 | 0.551875 | 0.511125 | . 75% 37.250000 | 3.000000 | 0.750031 | 0.769328 | . max 70.000000 | 12.000000 | 1.574250 | 1.374000 | . The statistics of the basic parameters considered. . d=df[[&#39;age&#39;,&#39;number of cycles&#39;, &#39;mean duration&#39;,&#39;avg time between cycles&#39;]] plt.figure(figsize=(8,8)) sns.heatmap(d.corr(), annot=True) plt . &lt;AxesSubplot:&gt; . From the correlation graph we can see that there is 22% positive correlation between mean duration of cyles and avg time between cycles. Also, there is a negative correlation of 25% between the number of cycles and mean duration of cycles, which is quite intuitive . sns.pairplot(df, kind=&quot;scatter&quot;, hue =&#39;status&#39;) . &lt;seaborn.axisgrid.PairGrid at 0x7ff2a6009f60&gt; . from scipy import stats result = stats.ttest_rel(df[&#39;avg time between cycles&#39;],df[&#39;mean duration&#39;]) print(result) . Ttest_relResult(statistic=-1.3490843600577016, pvalue=0.18038741481237539) . A paired t-test is performed. The degree of freedom here is 99.The p-value associated with the t-score, is greater than 0.05 which implies the mean difference between the variables is not different from 0. The variables are statistically redundant whihc is also proved by a small t-score as they have also have a reasonable amount of correlation (22%) . result = stats.ttest_rel(df[&#39;avg time between cycles&#39;],df[&#39;number of cycles&#39;]) print(result) . Ttest_relResult(statistic=-17.46913352274766, pvalue=5.26792739493255e-32) . A paired t-test is performed. The degree of freedom here is 99.The p-value associated with the t-score, is much lesser than 0.05 which implies the mean difference between the variables is ver much different from 0. The variables are statistically independent and relevent with very low correlation. . result = stats.ttest_rel(df[&#39;mean duration&#39;],df[&#39;number of cycles&#39;]) print(result) . Ttest_relResult(statistic=-16.500089266458964, pvalue=3.5818387170350524e-30) . op1=[&#39;covid&#39;] op2=[&#39;non_covid&#39;] covid_d=df[df[&#39;status&#39;].isin(op1)][&#39;mean duration&#39;] ncovid_d=df[df[&#39;status&#39;].isin(op2)][&#39;mean duration&#39;] result = stats.ttest_ind(covid_d,ncovid_d) print(result) . Ttest_indResult(statistic=0.22939968344720732, pvalue=0.8190363726412013) . Independent t-tests were performed for Covid and non-covid data over the mean-duration of cycles variables. A low t-score and high p-value of 0.82&gt;&gt;0.05, retains the null-hypothesis that both the groups for this variable have a similar statistical distribution. . op1=[&#39;covid&#39;] op2=[&#39;non_covid&#39;] covid_d=df[df[&#39;status&#39;].isin(op1)][&#39;avg time between cycles&#39;] ncovid_d=df[df[&#39;status&#39;].isin(op2)][&#39;avg time between cycles&#39;] result = stats.ttest_ind(covid_d,ncovid_d) print(result) . Ttest_indResult(statistic=-0.9531910950188025, pvalue=0.34283842530419995) . Independent t-tests were performed for Covid and non-covid data over the mean-duration of cycles variables. A low t-score and high p-value of 0.34&gt;&gt;0.05, retains the null-hypothesis that both the groups for this variable have a similar statistical distribution. Probably the 100 files chosen out of more than 400 is giving these statistics. If done on a larger dataset, better values could be observed. .",
            "url": "https://ananyamuguli.github.io/Blogs/2020/10/14/Cough-statisticalanalysis.html",
            "relUrl": "/2020/10/14/Cough-statisticalanalysis.html",
            "date": " • Oct 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ananyamuguli.github.io/Blogs/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ananyamuguli.github.io/Blogs/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ananyamuguli.github.io/Blogs/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ananyamuguli.github.io/Blogs/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}